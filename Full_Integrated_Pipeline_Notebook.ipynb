{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4379f791",
   "metadata": {},
   "source": [
    "# üì¶ Unified Notebook for Multi-label Email Classification\n",
    "This notebook combines all components from individual Python files into a single, editable environment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473d868c",
   "metadata": {},
   "source": [
    "## üîπ Config.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566dbd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    # Input Columns\n",
    "    TICKET_SUMMARY = 'Ticket Summary'\n",
    "    INTERACTION_CONTENT = 'Interaction content'\n",
    "\n",
    "    # Type Columns to test\n",
    "    TYPE_COLS = ['y2', 'y3', 'y4']\n",
    "    CLASS_COL = 'y2'\n",
    "    GROUPED = 'y1'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6556e04a",
   "metadata": {},
   "source": [
    "## üîπ Base Model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7866d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class BaseModel(ABC):\n",
    "    def __init__(self) -> None:\n",
    "        ...\n",
    "\n",
    "\n",
    "    @abstractmethod\n",
    "    def train(self) -> None:\n",
    "        \"\"\"\n",
    "        Train the model using ML Models for Multi-class and mult-label classification.\n",
    "        :params: df is essential, others are model specific\n",
    "        :return: classifier\n",
    "        \"\"\"\n",
    "        ...\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(self) -> int:\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        ...\n",
    "\n",
    "    @abstractmethod\n",
    "    def data_transform(self) -> None:\n",
    "        return\n",
    "\n",
    "    # def build(self, values) -> BaseModel:\n",
    "    def build(self, values={}):\n",
    "        values = values if isinstance(values, dict) else utils.string2any(values)\n",
    "        self.__dict__.update(self.defaults)\n",
    "        self.__dict__.update(values)\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320ab46a",
   "metadata": {},
   "source": [
    "## üîπ Preprocessing.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f65e7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import re\n",
    "from Config import *\n",
    "\n",
    "\n",
    "def get_input_data()->pd.DataFrame:\n",
    "    df1 = pd.read_csv(\"data//AppGallery.csv\", skipinitialspace=True)\n",
    "    df1.rename(columns={'Type 1': 'y1', 'Type 2': 'y2', 'Type 3': 'y3', 'Type 4': 'y4'}, inplace=True)\n",
    "    df2 = pd.read_csv(\"data//Purchasing.csv\", skipinitialspace=True)\n",
    "    df2.rename(columns={'Type 1': 'y1', 'Type 2': 'y2', 'Type 3': 'y3', 'Type 4': 'y4'}, inplace=True)\n",
    "    df = pd.concat([df1, df2])\n",
    "    df[Config.INTERACTION_CONTENT] = df[Config.INTERACTION_CONTENT].values.astype('U')\n",
    "    df[Config.TICKET_SUMMARY] = df[Config.TICKET_SUMMARY].values.astype('U')\n",
    "    df[\"y\"] = df[Config.CLASS_COL]\n",
    "    df = df.loc[(df[\"y\"] != '') & (~df[\"y\"].isna()),]\n",
    "    return df\n",
    "\n",
    "def de_duplication(data: pd.DataFrame):\n",
    "    data[\"ic_deduplicated\"] = \"\"\n",
    "\n",
    "    cu_template = {\n",
    "        \"english\":\n",
    "            [\"(?:Aspiegel|\\*\\*\\*\\*\\*\\(PERSON\\)) Customer Support team\\,?\",\n",
    "             \"(?:Aspiegel|\\*\\*\\*\\*\\*\\(PERSON\\)) SE is a company incorporated under the laws of Ireland with its headquarters in Dublin, Ireland\\.?\",\n",
    "             \"(?:Aspiegel|\\*\\*\\*\\*\\*\\(PERSON\\)) SE is the provider of Huawei Mobile Services to Huawei and Honor device owners in (?:Europe|\\*\\*\\*\\*\\*\\(LOC\\)), Canada, Australia, New Zealand and other countries\\.?\"]\n",
    "        ,\n",
    "        \"german\":\n",
    "            [\"(?:Aspiegel|\\*\\*\\*\\*\\*\\(PERSON\\)) Kundenservice\\,?\",\n",
    "             \"Die (?:Aspiegel|\\*\\*\\*\\*\\*\\(PERSON\\)) SE ist eine Gesellschaft nach irischem Recht mit Sitz in Dublin, Irland\\.?\",\n",
    "             \"(?:Aspiegel|\\*\\*\\*\\*\\*\\(PERSON\\)) SE ist der Anbieter von Huawei Mobile Services f√ºr Huawei- und Honor-Ger√§tebesitzer in Europa, Kanada, Australien, Neuseeland und anderen L√§ndern\\.?\"]\n",
    "        ,\n",
    "        \"french\":\n",
    "            [\"L'√©quipe d'assistance √† la client√®le d'Aspiegel\\,?\",\n",
    "             \"Die (?:Aspiegel|\\*\\*\\*\\*\\*\\(PERSON\\)) SE est une soci√©t√© de droit irlandais dont le si√®ge est √† Dublin, en Irlande\\.?\",\n",
    "             \"(?:Aspiegel|\\*\\*\\*\\*\\*\\(PERSON\\)) SE est le fournisseur de services mobiles Huawei aux propri√©taires d'appareils Huawei et Honor en Europe, au Canada, en Australie, en Nouvelle-Z√©lande et dans d'autres pays\\.?\"]\n",
    "        ,\n",
    "        \"spanish\":\n",
    "            [\"(?:Aspiegel|\\*\\*\\*\\*\\*\\(PERSON\\)) Soporte Servicio al Cliente\\,?\",\n",
    "             \"Die (?:Aspiegel|\\*\\*\\*\\*\\*\\(PERSON\\)) es una sociedad constituida en virtud de la legislaci√≥n de Irlanda con su sede en Dubl√≠n, Irlanda\\.?\",\n",
    "             \"(?:Aspiegel|\\*\\*\\*\\*\\*\\(PERSON\\)) SE es el proveedor de servicios m√≥viles de Huawei a los propietarios de dispositivos de Huawei y Honor en Europa, Canad√°, Australia, Nueva Zelanda y otros pa√≠ses\\.?\"]\n",
    "        ,\n",
    "        \"italian\":\n",
    "            [\"Il tuo team ad (?:Aspiegel|\\*\\*\\*\\*\\*\\(PERSON\\)),?\",\n",
    "             \"Die (?:Aspiegel|\\*\\*\\*\\*\\*\\(PERSON\\)) SE √® una societ√† costituita secondo le leggi irlandesi con sede a Dublino, Irlanda\\.?\",\n",
    "             \"(?:Aspiegel|\\*\\*\\*\\*\\*\\(PERSON\\)) SE √® il fornitore di servizi mobili Huawei per i proprietari di dispositivi Huawei e Honor in Europa, Canada, Australia, Nuova Zelanda e altri paesi\\.?\"]\n",
    "        ,\n",
    "        \"portguese\":\n",
    "            [\"(?:Aspiegel|\\*\\*\\*\\*\\*\\(PERSON\\)) Customer Support team,?\",\n",
    "             \"Die (?:Aspiegel|\\*\\*\\*\\*\\*\\(PERSON\\)) SE √© uma empresa constitu√≠da segundo as leis da Irlanda, com sede em Dublin, Irlanda\\.?\",\n",
    "             \"(?:Aspiegel|\\*\\*\\*\\*\\*\\(PERSON\\)) SE √© o provedor de Huawei Mobile Services para Huawei e Honor propriet√°rios de dispositivos na Europa, Canad√°, Austr√°lia, Nova Zel√¢ndia e outros pa√≠ses\\.?\"]\n",
    "        ,\n",
    "    }\n",
    "\n",
    "    cu_pattern = \"\"\n",
    "    for i in sum(list(cu_template.values()), []):\n",
    "        cu_pattern = cu_pattern + f\"({i})|\"\n",
    "    cu_pattern = cu_pattern[:-1]\n",
    "\n",
    "    # -------- email split template\n",
    "\n",
    "    pattern_1 = \"(From\\s?:\\s?xxxxx@xxxx.com Sent\\s?:.{30,70}Subject\\s?:)\"\n",
    "    pattern_2 = \"(On.{30,60}wrote:)\"\n",
    "    pattern_3 = \"(Re\\s?:|RE\\s?:)\"\n",
    "    pattern_4 = \"(\\*\\*\\*\\*\\*\\(PERSON\\) Support issue submit)\"\n",
    "    pattern_5 = \"(\\s?\\*\\*\\*\\*\\*\\(PHONE\\))*$\"\n",
    "\n",
    "    split_pattern = f\"{pattern_1}|{pattern_2}|{pattern_3}|{pattern_4}|{pattern_5}\"\n",
    "\n",
    "    # -------- start processing ticket data\n",
    "\n",
    "    tickets = data[\"Ticket id\"].value_counts()\n",
    "\n",
    "    for t in tickets.index:\n",
    "        #print(t)\n",
    "        df = data.loc[data['Ticket id'] == t,]\n",
    "\n",
    "        # for one ticket content data\n",
    "        ic_set = set([])\n",
    "        ic_deduplicated = []\n",
    "        for ic in df[Config.INTERACTION_CONTENT]:\n",
    "\n",
    "            # print(ic)\n",
    "\n",
    "            ic_r = re.split(split_pattern, ic)\n",
    "            # ic_r = sum(ic_r, [])\n",
    "\n",
    "            ic_r = [i for i in ic_r if i is not None]\n",
    "\n",
    "            # replace split patterns\n",
    "            ic_r = [re.sub(split_pattern, \"\", i.strip()) for i in ic_r]\n",
    "\n",
    "            # replace customer template\n",
    "            ic_r = [re.sub(cu_pattern, \"\", i.strip()) for i in ic_r]\n",
    "\n",
    "            ic_current = []\n",
    "            for i in ic_r:\n",
    "                if len(i) > 0:\n",
    "                    # print(i)\n",
    "                    if i not in ic_set:\n",
    "                        ic_set.add(i)\n",
    "                        i = i + \"\\n\"\n",
    "                        ic_current = ic_current + [i]\n",
    "\n",
    "            #print(ic_current)\n",
    "            ic_deduplicated = ic_deduplicated + [' '.join(ic_current)]\n",
    "        data.loc[data[\"Ticket id\"] == t, \"ic_deduplicated\"] = ic_deduplicated\n",
    "    data.to_csv('out.csv')\n",
    "    data[Config.INTERACTION_CONTENT] = data['ic_deduplicated']\n",
    "    data = data.drop(columns=['ic_deduplicated'])\n",
    "    return data\n",
    "\n",
    "def noise_remover(df: pd.DataFrame):\n",
    "    noise = \"(sv\\s*:)|(wg\\s*:)|(ynt\\s*:)|(fw(d)?\\s*:)|(r\\s*:)|(re\\s*:)|(\\[|\\])|(aspiegel support issue submit)|(null)|(nan)|((bonus place my )?support.pt Ëá™Âä®ÂõûÂ§ç:)\"\n",
    "    df[Config.TICKET_SUMMARY] = df[Config.TICKET_SUMMARY].str.lower().replace(noise, \" \", regex=True).replace(r'\\s+', ' ', regex=True).str.strip()\n",
    "    df[Config.INTERACTION_CONTENT] = df[Config.INTERACTION_CONTENT].str.lower()\n",
    "    noise_1 = [\n",
    "        \"(from :)|(subject :)|(sent :)|(r\\s*:)|(re\\s*:)\",\n",
    "        \"(january|february|march|april|may|june|july|august|september|october|november|december)\",\n",
    "        \"(jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec)\",\n",
    "        \"(monday|tuesday|wednesday|thursday|friday|saturday|sunday)\",\n",
    "        \"\\d{2}(:|.)\\d{2}\",\n",
    "        \"(xxxxx@xxxx\\.com)|(\\*{5}\\([a-z]+\\))\",\n",
    "        \"dear ((customer)|(user))\",\n",
    "        \"dear\",\n",
    "        \"(hello)|(hallo)|(hi )|(hi there)\",\n",
    "        \"good morning\",\n",
    "        \"thank you for your patience ((during (our)? investigation)|(and cooperation))?\",\n",
    "        \"thank you for contacting us\",\n",
    "        \"thank you for your availability\",\n",
    "        \"thank you for providing us this information\",\n",
    "        \"thank you for contacting\",\n",
    "        \"thank you for reaching us (back)?\",\n",
    "        \"thank you for patience\",\n",
    "        \"thank you for (your)? reply\",\n",
    "        \"thank you for (your)? response\",\n",
    "        \"thank you for (your)? cooperation\",\n",
    "        \"thank you for providing us with more information\",\n",
    "        \"thank you very kindly\",\n",
    "        \"thank you( very much)?\",\n",
    "        \"i would like to follow up on the case you raised on the date\",\n",
    "        \"i will do my very best to assist you\"\n",
    "        \"in order to give you the best solution\",\n",
    "        \"could you please clarify your request with following information:\"\n",
    "        \"in this matter\",\n",
    "        \"we hope you(( are)|('re)) doing ((fine)|(well))\",\n",
    "        \"i would like to follow up on the case you raised on\",\n",
    "        \"we apologize for the inconvenience\",\n",
    "        \"sent from my huawei (cell )?phone\",\n",
    "        \"original message\",\n",
    "        \"customer support team\",\n",
    "        \"(aspiegel )?se is a company incorporated under the laws of ireland with its headquarters in dublin, ireland.\",\n",
    "        \"(aspiegel )?se is the provider of huawei mobile services to huawei and honor device owners in\",\n",
    "        \"canada, australia, new zealand and other countries\",\n",
    "        \"\\d+\",\n",
    "        \"[^0-9a-zA-Z]+\",\n",
    "        \"(\\s|^).(\\s|$)\"]\n",
    "    for noise in noise_1:\n",
    "        #print(noise)\n",
    "        df[Config.INTERACTION_CONTENT] = df[Config.INTERACTION_CONTENT].replace(noise, \" \", regex=True)\n",
    "    df[Config.INTERACTION_CONTENT] = df[Config.INTERACTION_CONTENT].replace(r'\\s+', ' ', regex=True).str.strip()\n",
    "    #print(df.y1.value_counts())\n",
    "    good_y1 = df.y1.value_counts()[df.y1.value_counts() > 10].index\n",
    "    df = df.loc[df.y1.isin(good_y1)]\n",
    "    #print(df.shape)\n",
    "    return df\n",
    "\n",
    "def translate_to_en(texts:list[str]):\n",
    "    import stanza\n",
    "    from stanza.pipeline.core import DownloadMethod\n",
    "    from transformers import pipeline\n",
    "    from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\n",
    "\n",
    "    t2t_m = \"facebook/m2m100_418M\"\n",
    "    t2t_pipe = pipeline(task='text2text-generation', model=t2t_m)\n",
    "\n",
    "    model = M2M100ForConditionalGeneration.from_pretrained(t2t_m)\n",
    "    tokenizer = M2M100Tokenizer.from_pretrained(t2t_m)\n",
    "\n",
    "    nlp_stanza = stanza.Pipeline(lang=\"multilingual\", processors=\"langid\",\n",
    "                                 download_method=DownloadMethod.REUSE_RESOURCES)\n",
    "    text_en_l = []\n",
    "    for text in texts:\n",
    "        if text == \"\":\n",
    "            text_en_l = text_en_l + [text]\n",
    "            continue\n",
    "\n",
    "        doc = nlp_stanza(text)\n",
    "        #print(doc.lang)\n",
    "        if doc.lang == \"en\":\n",
    "            text_en_l = text_en_l + [text]\n",
    "            # print(text)\n",
    "        else:\n",
    "            # convert to model supported language code\n",
    "            # https://stanfordnlp.github.io/stanza/available_models.html\n",
    "            # https://github.com/huggingface/transformers/blob/main/src/transformers/models/m2m_100/tokenization_m2m_100.py\n",
    "            lang = doc.lang\n",
    "            if lang == \"fro\":  # fro = Old French\n",
    "                lang = \"fr\"\n",
    "            elif lang == \"la\":  # latin\n",
    "                lang = \"it\"\n",
    "            elif lang == \"nn\":  # Norwegian (Nynorsk)\n",
    "                lang = \"no\"\n",
    "            elif lang == \"kmr\":  # Kurmanji\n",
    "                lang = \"tr\"\n",
    "\n",
    "            case = 2\n",
    "\n",
    "            if case == 1:\n",
    "                text_en = t2t_pipe(text, forced_bos_token_id=t2t_pipe.tokenizer.get_lang_id(lang='en'))\n",
    "                text_en = text_en[0]['generated_text']\n",
    "            elif case == 2:\n",
    "                tokenizer.src_lang = lang\n",
    "                encoded_hi = tokenizer(text, return_tensors=\"pt\")\n",
    "                generated_tokens = model.generate(**encoded_hi, forced_bos_token_id=tokenizer.get_lang_id(\"en\"))\n",
    "                text_en = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "                text_en = text_en[0]\n",
    "            else:\n",
    "                text_en = text\n",
    "            text_en_l = text_en_l + [text_en]\n",
    "            #print(text)\n",
    "            #print(text_en)\n",
    "    return text_en_l\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd7049e",
   "metadata": {},
   "source": [
    "## üîπ Embeddings.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6badf000",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from Config import *\n",
    "import random\n",
    "seed =0\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "def get_tfidf_embd(df:pd.DataFrame):\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    tfidfconverter = TfidfVectorizer(max_features=2000, min_df=4, max_df=0.90)\n",
    "    data = df[Config.TICKET_SUMMARY] + ' ' + df[Config.INTERACTION_CONTENT]\n",
    "    X = tfidfconverter.fit_transform(data).toarray()\n",
    "    return X\n",
    "\n",
    "def combine_embd(X1, X2):\n",
    "    return np.concatenate((X1, X2), axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ebdc0a",
   "metadata": {},
   "source": [
    "## üîπ Data Model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0f3b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from Config import *\n",
    "import random\n",
    "seed =0\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "class Data():\n",
    "    def __init__(self,\n",
    "                 X: np.ndarray,\n",
    "                 df: pd.DataFrame) -> None:\n",
    "\n",
    "        y = df.y.to_numpy()\n",
    "        y_series = pd.Series(y)\n",
    "\n",
    "        good_y_value = y_series.value_counts()[y_series.value_counts() >= 3].index\n",
    "\n",
    "        if len(good_y_value)<1:\n",
    "            print(\"None of the class have more than 3 records: Skipping ...\")\n",
    "            self.X_train = None\n",
    "            return\n",
    "\n",
    "        y_good = y[y_series.isin(good_y_value)]\n",
    "        X_good = X[y_series.isin(good_y_value)]\n",
    "\n",
    "        new_test_size = X.shape[0] * 0.2 / X_good.shape[0]\n",
    "\n",
    "\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test= train_test_split(X_good, y_good, test_size=new_test_size, random_state=0, stratify=y_good)\n",
    "        self.y = y_good\n",
    "        self.classes = good_y_value\n",
    "        self.embeddings = X\n",
    "\n",
    "\n",
    "    def get_type(self):\n",
    "        return  self.y\n",
    "    def get_X_train(self):\n",
    "        return  self.X_train\n",
    "    def get_X_test(self):\n",
    "        return  self.X_test\n",
    "    def get_type_y_train(self):\n",
    "        return  self.y_train\n",
    "    def get_type_y_test(self):\n",
    "        return  self.y_test\n",
    "    def get_train_df(self):\n",
    "        return  self.train_df\n",
    "    def get_embeddings(self):\n",
    "        return  self.embeddings\n",
    "    def get_type_test_df(self):\n",
    "        return  self.test_df\n",
    "    def get_X_DL_test(self):\n",
    "        return self.X_DL_test\n",
    "    def get_X_DL_train(self):\n",
    "        return self.X_DL_train\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3647971d",
   "metadata": {},
   "source": [
    "## üîπ Random Forest Model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f72631",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from model.base import BaseModel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from numpy import *\n",
    "import random\n",
    "num_folds = 0\n",
    "seed =0\n",
    "# Data\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "\n",
    "\n",
    "class RandomForest(BaseModel):\n",
    "    def __init__(self,\n",
    "                 model_name: str,\n",
    "                 embeddings: np.ndarray,\n",
    "                 y: np.ndarray) -> None:\n",
    "        super(RandomForest, self).__init__()\n",
    "        self.model_name = model_name\n",
    "        self.embeddings = embeddings\n",
    "        self.y = y\n",
    "        self.mdl = RandomForestClassifier(n_estimators=1000, random_state=seed, class_weight='balanced_subsample')\n",
    "        self.predictions = None\n",
    "        self.data_transform()\n",
    "\n",
    "    def train(self, data) -> None:\n",
    "        self.mdl = self.mdl.fit(data.X_train, data.y_train)\n",
    "\n",
    "    def predict(self, X_test: pd.Series):\n",
    "        predictions = self.mdl.predict(X_test)\n",
    "        self.predictions = predictions\n",
    "\n",
    "    def print_results(self, data):\n",
    "        print(classification_report(data.y_test, self.predictions))\n",
    "\n",
    "\n",
    "    def data_transform(self) -> None:\n",
    "        ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f0dae0",
   "metadata": {},
   "source": [
    "## üîπ Modelling Utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe3272f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.randomforest import RandomForest\n",
    "\n",
    "\n",
    "\n",
    "def model_predict(data, df, name):\n",
    "    results = []\n",
    "    print(\"RandomForest\")\n",
    "    model = RandomForest(\"RandomForest\", data.get_embeddings(), data.get_type())\n",
    "    model.train(data)\n",
    "    model.predict(data.X_test)\n",
    "    model.print_results(data)\n",
    "\n",
    "\n",
    "def model_evaluate(model, data):\n",
    "    model.print_results(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f192af",
   "metadata": {},
   "source": [
    "## üîπ Main Pipeline.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21a699d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess import *\n",
    "from embeddings import *\n",
    "from modelling.modelling import *\n",
    "from modelling.data_model import *\n",
    "import random\n",
    "seed =0\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    #load the input data\n",
    "    df = get_input_data()\n",
    "    return  df\n",
    "\n",
    "def preprocess_data(df):\n",
    "    # De-duplicate input data\n",
    "    df =  de_duplication(df)\n",
    "    # remove noise in input data\n",
    "    df = noise_remover(df)\n",
    "    # translate data to english\n",
    "    # df[Config.TICKET_SUMMARY] = translate_to_en(df[Config.TICKET_SUMMARY].tolist())\n",
    "    return df\n",
    "\n",
    "def get_embeddings(df:pd.DataFrame):\n",
    "    X = get_tfidf_embd(df)  # get tf-idf embeddings\n",
    "    return X, df\n",
    "\n",
    "def get_data_object(X: np.ndarray, df: pd.DataFrame):\n",
    "    return Data(X, df)\n",
    "\n",
    "def perform_modelling(data: Data, df: pd.DataFrame, name):\n",
    "    model_predict(data, df, name)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    df = load_data()\n",
    "    df = preprocess_data(df)\n",
    "    df[Config.INTERACTION_CONTENT] = df[Config.INTERACTION_CONTENT].values.astype('U')\n",
    "    df[Config.TICKET_SUMMARY] = df[Config.TICKET_SUMMARY].values.astype('U')\n",
    "    grouped_df = df.groupby(Config.GROUPED)\n",
    "    for name, group_df in grouped_df:\n",
    "        print(name)\n",
    "        X, group_df = get_embeddings(group_df)\n",
    "        data = get_data_object(X, group_df)\n",
    "        perform_modelling(data, group_df, name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d26ae10",
   "metadata": {},
   "source": [
    "## üöÄ Run the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c4af51",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ‚ñ∂Ô∏è Execute Pipeline (from main.py)\n",
    "if __name__ == \"__main__\":\n",
    "    run_pipeline()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
